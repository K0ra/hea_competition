{
  "experiment_name": "mlp_large_all",
  "model_type": "mlp",
  "model_params": {
    "hidden_layer_sizes": [
      256,
      128,
      64
    ],
    "activation": "relu",
    "solver": "adam",
    "alpha": 0.0001,
    "batch_size": 512,
    "learning_rate_init": 0.0003,
    "max_iter": 200,
    "early_stopping": true,
    "validation_fraction": 0.1
  },
  "feature_subset": "all",
  "n_features": 288,
  "use_smote": false,
  "class_weight": 1.5009628654876914,
  "training_time_sec": 77.6074149608612,
  "metrics": {
    "ROC_AUC": 0.6782,
    "PR_AUC": 0.1439,
    "F2_score": 0.2035,
    "best_threshold": 0.1,
    "n_samples": 25089,
    "n_positive": 1706
  }
}